"use strict";(self.webpackChunkrobot_intel_fl24=self.webpackChunkrobot_intel_fl24||[]).push([[721],{1145:function(t,e,a){a.r(e),a.d(e,{frontMatter:function(){return o},contentTitle:function(){return d},metadata:function(){return m},toc:function(){return p},default:function(){return s}});var n=a(7462),r=a(3366),l=(a(7294),a(3905)),i=["components"],o={title:"Syllabus"},d=void 0,m={type:"mdx",permalink:"/decision-making-sp25/syllabus",source:"@site/src/pages/syllabus.md"},p=[{value:"Course Content",id:"course-content",children:[],level:2}],k={toc:p};function s(t){var e=t.components,a=(0,r.Z)(t,i);return(0,l.kt)("wrapper",(0,n.Z)({},k,a,{components:e,mdxType:"MDXLayout"}),(0,l.kt)("h2",{id:"course-content"},"Course Content"),(0,l.kt)("table",null,(0,l.kt)("thead",{parentName:"table"},(0,l.kt)("tr",{parentName:"thead"},(0,l.kt)("th",{parentName:"tr",align:"left"},"Date"),(0,l.kt)("th",{parentName:"tr",align:"left"},"Lectures"),(0,l.kt)("th",{parentName:"tr",align:"left"},"Contents"),(0,l.kt)("th",{parentName:"tr",align:"left"},"Readings"))),(0,l.kt)("tbody",{parentName:"table"},(0,l.kt)("tr",{parentName:"tbody"},(0,l.kt)("td",{parentName:"tr",align:"left"},"1/24"),(0,l.kt)("td",{parentName:"tr",align:"left"},"Introduction to Decision Making"),(0,l.kt)("td",{parentName:"tr",align:"left"},"Part 1: Intelligence and decision making ",(0,l.kt)("br",null)," Part 2: What is this class about?"),(0,l.kt)("td",{parentName:"tr",align:"left"},"1. ",(0,l.kt)("a",{parentName:"td",href:"https://wi.mit.edu/unusual-labmates-how-c-elegans-wormed-its-way-science-stardom"},"Unusual Labmates: how C. elegans wormed its way into science stardom"),"  ",(0,l.kt)("br",null)," 2. ",(0,l.kt)("a",{parentName:"td",href:"https://www.dummies.com/book/academics-the-arts/science/neuroscience/neuroscience-for-dummies-2nd-edition-282419/"},"Neuroscience For Dummies")," ",(0,l.kt)("br",null)," 3. ",(0,l.kt)("a",{parentName:"td",href:"https://www.amazon.com/Brief-History-Intelligence-Humans-Breakthroughs/dp/0063286343"},"A Brief History of Intelligence: Evolution, AI, and the Five Breakthroughs That Made Our Brains"))),(0,l.kt)("tr",{parentName:"tbody"},(0,l.kt)("td",{parentName:"tr",align:"left"},"1/31"),(0,l.kt)("td",{parentName:"tr",align:"left"},"Supervised Learning for Decision Making"),(0,l.kt)("td",{parentName:"tr",align:"left"},"Part 1: Training Neural Networks ",(0,l.kt)("br",null)," Part 2: Variants of Behavior Cloning policies"),(0,l.kt)("td",{parentName:"tr",align:"left"},"1. Behavior Cloning (ALVINN) ",(0,l.kt)("br",null)," 2. Variational Autoencoder ",(0,l.kt)("br",null)," 3. Generative Adversarial Networks ",(0,l.kt)("br",null)," 4. Case study papers: VINN, RT-1, Dobb-E, Implicit BC, BeT, C-BeT, Diffusion Policy ",(0,l.kt)("br",null)," 5. ",(0,l.kt)("a",{parentName:"td",href:"https://supervised-robot-learning.github.io/"},"Supervised Policy Learning for Real Robots"))),(0,l.kt)("tr",{parentName:"tbody"},(0,l.kt)("td",{parentName:"tr",align:"left"},"2/7"),(0,l.kt)("td",{parentName:"tr",align:"left"},"Tutorial: Supervised Learning for Decision Making"),(0,l.kt)("td",{parentName:"tr",align:"left"},"Setting up decision making environments and model training"),(0,l.kt)("td",{parentName:"tr",align:"left"},(0,l.kt)("a",{parentName:"td",href:"https://drive.google.com/drive/folders/1izZY_IQr6vZY-EhP0avbH2nNxc2wXxA-?usp=sharing"},"Resources"))),(0,l.kt)("tr",{parentName:"tbody"},(0,l.kt)("td",{parentName:"tr",align:"left"},"2/14"),(0,l.kt)("td",{parentName:"tr",align:"left"},"Multi-Armed Bandits"),(0,l.kt)("td",{parentName:"tr",align:"left"},"Part 1: Formalism for Bandit problem ",(0,l.kt)("br",null)," Part 2: Algorithms for Bandit problems"),(0,l.kt)("td",{parentName:"tr",align:"left"},"1. ",(0,l.kt)("a",{parentName:"td",href:"https://ianosband.com/2015/07/28/Beat-the-bandit.html"},"Can you beat the bandit?")," ",(0,l.kt)("br",null)," 2. ",(0,l.kt)("a",{parentName:"td",href:"https://learnforeverlearn.com/bandits/"},"Bayesian Bandit Explorer"))),(0,l.kt)("tr",{parentName:"tbody"},(0,l.kt)("td",{parentName:"tr",align:"left"},"2/21"),(0,l.kt)("td",{parentName:"tr",align:"left"},"Guest Lecture: Mahi Shafiullah"),(0,l.kt)("td",{parentName:"tr",align:"left"},"Examples of policy learning working in the real world"),(0,l.kt)("td",{parentName:"tr",align:"left"})),(0,l.kt)("tr",{parentName:"tbody"},(0,l.kt)("td",{parentName:"tr",align:"left"},"2/28"),(0,l.kt)("td",{parentName:"tr",align:"left"},"Markov Decision Process and simple RL methods"),(0,l.kt)("td",{parentName:"tr",align:"left"},"Part 1: Motivation and formalism ",(0,l.kt)("br",null)," Part 2: Core concepts of value and policy iteration"),(0,l.kt)("td",{parentName:"tr",align:"left"},"Book: ",(0,l.kt)("a",{parentName:"td",href:"http://incompleteideas.net/book/the-book-2nd.html"},"Reinforcement Learning: An Introduction"))),(0,l.kt)("tr",{parentName:"tbody"},(0,l.kt)("td",{parentName:"tr",align:"left"},"3/7"),(0,l.kt)("td",{parentName:"tr",align:"left"},"Q-learning: from Tables to Atari"),(0,l.kt)("td",{parentName:"tr",align:"left"},"Part 1: Why Q function? ",(0,l.kt)("br",null)," Part 2: Deep Q functions: What goes wrong and how to make them work? ",(0,l.kt)("br",null)," Part 3: Variants of DQN"),(0,l.kt)("td",{parentName:"tr",align:"left"})),(0,l.kt)("tr",{parentName:"tbody"},(0,l.kt)("td",{parentName:"tr",align:"left"},"3/14"),(0,l.kt)("td",{parentName:"tr",align:"left"},"Policy Optimization"),(0,l.kt)("td",{parentName:"tr",align:"left"},"Part 1: MC-based optimization (CEM) ",(0,l.kt)("br",null)," Part 2: Differentiable versions (REINFORCE) ",(0,l.kt)("br",null)," Part 3: Trust region / proximal policy optimization"),(0,l.kt)("td",{parentName:"tr",align:"left"})),(0,l.kt)("tr",{parentName:"tbody"},(0,l.kt)("td",{parentName:"tr",align:"left"},"3/21"),(0,l.kt)("td",{parentName:"tr",align:"left"},"Tutorial: Visual and Temporal Policy Learning"),(0,l.kt)("td",{parentName:"tr",align:"left"}),(0,l.kt)("td",{parentName:"tr",align:"left"})),(0,l.kt)("tr",{parentName:"tbody"},(0,l.kt)("td",{parentName:"tr",align:"left"},"3/28"),(0,l.kt)("td",{parentName:"tr",align:"left"},"SPRING BREAK"),(0,l.kt)("td",{parentName:"tr",align:"left"}),(0,l.kt)("td",{parentName:"tr",align:"left"})),(0,l.kt)("tr",{parentName:"tbody"},(0,l.kt)("td",{parentName:"tr",align:"left"},"4/4"),(0,l.kt)("td",{parentName:"tr",align:"left"},"Guest Lecture: TBD"),(0,l.kt)("td",{parentName:"tr",align:"left"}),(0,l.kt)("td",{parentName:"tr",align:"left"})),(0,l.kt)("tr",{parentName:"tbody"},(0,l.kt)("td",{parentName:"tr",align:"left"},"4/11"),(0,l.kt)("td",{parentName:"tr",align:"left"},"Decision Making with World Models"),(0,l.kt)("td",{parentName:"tr",align:"left"},"Part 1: Classical approaches (LQR / iLQR / DDP) ",(0,l.kt)("br",null)," Part 2: Model-based RL ",(0,l.kt)("br",null)," Part 3: case study: Dreamer v3"),(0,l.kt)("td",{parentName:"tr",align:"left"})),(0,l.kt)("tr",{parentName:"tbody"},(0,l.kt)("td",{parentName:"tr",align:"left"},"4/18"),(0,l.kt)("td",{parentName:"tr",align:"left"},"Decision Making with Tree Search"),(0,l.kt)("td",{parentName:"tr",align:"left"},"MCTC (AlphaGo, AlphaZero)"),(0,l.kt)("td",{parentName:"tr",align:"left"})),(0,l.kt)("tr",{parentName:"tbody"},(0,l.kt)("td",{parentName:"tr",align:"left"},"4/25"),(0,l.kt)("td",{parentName:"tr",align:"left"},"Revisiting Decision Making with Expert Data"),(0,l.kt)("td",{parentName:"tr",align:"left"},"Inverse RL and offline RL"),(0,l.kt)("td",{parentName:"tr",align:"left"})),(0,l.kt)("tr",{parentName:"tbody"},(0,l.kt)("td",{parentName:"tr",align:"left"},"5/2"),(0,l.kt)("td",{parentName:"tr",align:"left"},"Course Project Presentations"),(0,l.kt)("td",{parentName:"tr",align:"left"}),(0,l.kt)("td",{parentName:"tr",align:"left"})))))}s.isMDXComponent=!0}}]);